+++
title = "GenAI Research"
weight = 2
sort_by = "weight"
insert_anchor_links = "right"
+++

## A Curated Collection of Generative AI Research & Tools

Welcome to a resource for Generative AI: from practical development tools to cutting-edge research papers. This section covers:

- **GenAI Agents Environment**: CLI agents, AI editors, MCP servers, and tools for your development workflow
- **Agents**: Research on autonomous AI agents and multi-agent systems
- **Research Papers**: Curated summaries of influential papers on training, optimization, prompting, and more

Whether you're setting up your AI development environment, exploring agent architectures, or diving into the latest research, this collection is your gateway to the world of generative AI.

## Research Papers Summary and Insights

Below is the list of selected research papers that have significantly contributed to the field of Generative AI and Large Language Models. Each entry includes a brief summary and an arXiv link to the full paper for a comprehensive read.

### Pretraining / Fine-Tuning

- [Survey] Instruction Tuning for Large Language Models

-  [RA-DIT] Retrieval-Augmented Dual Instruction Tuning

- [Sequential Monte Carlo] Steering of LLMs using Probabilistic Programs

### Agents

- [SICA] A Self-Improving Coding Agent

- [AutoGen] Enabling Next-Gen LLM Applications via Multi-Agent Chat

- [RetroFormer] Retrospective LL Agents with Policy Gradient Optimization

### Optimization

- [LLM-in-a-Flash] Efficient LLM Inference with Limited Memory

- [RoPE] RoFormer: Enhanced Transformer with Rotary Position Embedding

- [LORA] LOw-RAnk Adaptation of LLM

- [Speculative] Fast Inference from Transformers via Speculative Decoding


- [GQA] Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints


- [Multi-Heads Sharing] Fast Transformer Decoding: One Write-Head is All You Need

- [MoE] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-Of-Experts Layer


- [MoE] Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for LLM

### Prompting

- [MedPrompt] Can Generalist Foundation Models Outcompete Special-Purpose Tuning?

- [URIAL] The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning

- [CoVE] Chain-of-Verification Reduces Hallucinations in LLM Models

### Benchmarks & Evaluation

- [Benchmark] Generating Benchmarks for Factuality Evaluation of Language Models

### Multi-Modal / Vision

- [Point-E] A System for Generating 3D Point Clouds from Complex Prompts

- [CLIP] Connecting text and images

### Models

- [Gemini] A Family of Highly Capable Multimodal Models

---

As we embark on this scholarly expedition, remember that this is just the beginning. The field of GenAI is ever-evolving, with new discoveries and insights emerging regularly. Keep this page bookmarked, and revisit often to stay updated with the latest research that shapes the future of generative AI.
