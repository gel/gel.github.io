<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="x-ua-compatible" content="ie=edge" />
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, viewport-fit=cover"
    />

    <style>
      :root {
        --accent-color: #05a081;
        --accent-color-light: #82d0c0;
        --accent-overlay-color: #fff;
        --body-bg: #fff;
        --body-color: #000;
        --heading-color: #000;
        --table-bg-even: #f3f3f3;
        --table-border-bottom: #dddddd;
      }
      
    </style>

    <meta name="theme-color" content="#05a081" />

    
      <link rel="icon" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.5bc282474aa9c014.jpg" />
      <link rel="apple-touch-icon" sizes="48x48" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.5bc282474aa9c014.jpg" />
      <link rel="apple-touch-icon" sizes="72x72" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.fe16e0bb11047769.jpg" />
      <link rel="apple-touch-icon" sizes="96x96" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.605594d8be6a111f.jpg" />
      <link rel="apple-touch-icon" sizes="144x144" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.3d1b2eeabf8eca47.jpg" />
      <link rel="apple-touch-icon" sizes="192x192" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.b6439cbc27d70267.jpg" />
      <link rel="apple-touch-icon" sizes="256x256" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.e69fb5473e9ea744.jpg" />
      <link rel="apple-touch-icon" sizes="384x384" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.d03b11f47410640c.jpg" />
      <link rel="apple-touch-icon" sizes="512x512" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.d847062c67ed9f5e.jpg" />
      
    

    
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-JD979V1BWS"></script>
      <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-JD979V1BWS');
      </script>
    

    <meta property="og:type" content="website">

    <meta name="twitter:card" content="summary">

    

    

    
      
        <meta name="description" content="" />
        <meta name="twitter:description" content="">
      
    

    
      <meta name="twitter:title" content="LLM Optimization">
    

    
      <link rel="prerender" href="&#x2F;blog&#x2F;" />
    
      <link rel="prerender" href="&#x2F;books&#x2F;" />
    
      <link rel="prerender" href="&#x2F;interviews&#x2F;" />
    
      <link rel="prerender" href="&#x2F;generative-ai&#x2F;" />
    

    <link rel="prefetch" href="https:&#x2F;&#x2F;gel.github.io&#x2F;processed_images&#x2F;iconweb.db45dd197188a7fb.jpg" />

    <title>
      
    
    
    
    Gal's 
     
    
        Generative AI
    
  Book - LLM Optimization
    

    </title>

    
    
<link rel="stylesheet" href="https://gel.github.io/book.css">
      


     





  </head>
  <body>
    

    <main>
    
        <div class="menu">
            
            
            <nav role="navigation">
                <ul>
                    <li>
                        <a href="/">
                            ‚Üê Back to Blog
                        </a>
                    </li>
                    
                        
                        
                        
                        
                            
                            <li >
                                
                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;1-intro&#x2F;">
                                    <strong>1.</strong>
                                    Generative AI Foundations
                                </a>
                                
                                    <ul>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;1-intro&#x2F;1-llm-survey&#x2F;">
                                                    <strong>1.1.</strong>
                                                    LLM Survey
                                                </a>
                                            </li>
                                        
                                    </ul>
                                
                            </li>
                        
                            
                            <li >
                                
                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;">
                                    <strong>2.</strong>
                                    LLM Research
                                </a>
                                
                                    <ul>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;1-llm-pretraining-finetuning&#x2F;">
                                                    <strong>2.1.</strong>
                                                    LLM Pretraining &amp; Fine-tuning
                                                </a>
                                            </li>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;2-llm-agents&#x2F;">
                                                    <strong>2.2.</strong>
                                                    LLM Agents
                                                </a>
                                            </li>
                                        
                                            <li class="active">
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;3-llm-optimization&#x2F;">
                                                    <strong>2.3.</strong>
                                                    LLM Optimization
                                                </a>
                                            </li>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;4-llm-prompting&#x2F;">
                                                    <strong>2.4.</strong>
                                                    LLM Prompting
                                                </a>
                                            </li>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;5-llm-benchmarks&#x2F;">
                                                    <strong>2.5.</strong>
                                                    LLM Benchmarks &amp; Evaluations
                                                </a>
                                            </li>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;6-llm-multimodal&#x2F;">
                                                    <strong>2.6.</strong>
                                                    LLM Multi-Modal
                                                </a>
                                            </li>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;7-llm-models&#x2F;">
                                                    <strong>2.7.</strong>
                                                    LLM Models
                                                </a>
                                            </li>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;8-llm-security&#x2F;">
                                                    <strong>2.8.</strong>
                                                    LLM Security &amp; Safety
                                                </a>
                                            </li>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;2-llm-research&#x2F;9-llm-architecture&#x2F;">
                                                    <strong>2.9.</strong>
                                                    LLM Architecture
                                                </a>
                                            </li>
                                        
                                    </ul>
                                
                            </li>
                        
                            
                            <li >
                                
                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;3-llm-implementation&#x2F;">
                                    <strong>3.</strong>
                                    LLM Implementation
                                </a>
                                
                                    <ul>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;3-llm-implementation&#x2F;1-datasets&#x2F;">
                                                    <strong>3.1.</strong>
                                                    Datasets
                                                </a>
                                            </li>
                                        
                                            <li >
                                                <a href="https:&#x2F;&#x2F;gel.github.io&#x2F;generative-ai&#x2F;3-llm-implementation&#x2F;2-llms&#x2F;">
                                                    <strong>3.2.</strong>
                                                    Large-Language-Models
                                                </a>
                                            </li>
                                        
                                    </ul>
                                
                            </li>
                                           
                    
                </ul>
            </nav>
            
            
        </div>

        <div class="page">
            <div class="page__header">
                <div class="menu-icon">
                    <span></span>
                    <span></span>
                    <span></span>
                </div>
                
                <span class="search-icon">üîé</span>
                
            </div>

            <div class="page__content">
                
                <div class="search-container">
                    <input id="search" type="search" placeholder="Search..">
                    <div class="search-results">
                        <div class="search-results__header"></div>
                        <ul class="search-results__items"></ul>
                    </div>
                </div>
                
                <div class="book-content">
                    
    <h1>LLM Optimization</h1>
    <h3 id="mixture-of-depths-dynamically-allocating-compute-in-transformer-based-lms">[Mixture-of-Depths] Dynamically allocating compute in transformer-based LMs<a class="zola-anchor" href="#mixture-of-depths-dynamically-allocating-compute-in-transformer-based-lms" aria-label="Anchor link for: mixture-of-depths-dynamically-allocating-compute-in-transformer-based-lms">üîó</a></h3>
<p>Arxiv: <a href="https://arxiv.org/abs/2404.02258">https://arxiv.org/abs/2404.02258</a> <em>2 Apr 2024 <strong>DeepMind</strong></em></p>
<p>The benefit of the approach is the ability to set a compute budget and then based on it enforce limits - for example: enforce how many tokens can participate in block computations. Therefore, in order to avoid performance degradation the challenge becomes how to choose the right tokens for processing.</p>
<h3 id="offload-moe-fast-inference-of-moe-language-models-with-offloading">[Offload-MoE] Fast Inference of MoE Language Models with Offloading<a class="zola-anchor" href="#offload-moe-fast-inference-of-moe-language-models-with-offloading" aria-label="Anchor link for: offload-moe-fast-inference-of-moe-language-models-with-offloading">üîó</a></h3>
<p>Arxiv: <a href="https://arxiv.org/abs/2312.17238">https://arxiv.org/abs/2312.17238</a> <em>28 Dec 2023 <strong>Yandex</strong></em></p>
<p>We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLM.</p>
<p>Technique 1: LRU Caching</p>
<ul>
<li>LRU is a simple strategy that doesn't consider factors like expert activation frequencies</li>
<li>Keep active experts in GPU memory as a &quot;cache&quot; for future tokens</li>
<li>If same experts are activated again, they're available instantaneously</li>
<li>For simplicity, always keep k least recently used experts as cache</li>
<li>If k &gt; number of active experts, cache saves experts from multiple previous tokens</li>
<li>Same number of cached experts maintained for each MoE layer</li>
</ul>
<p>Technique 2: Speculative Loading</p>
<ul>
<li>Prefetch next set of experts speculatively while processing previous layer</li>
<li>Guess likely next experts based on previous layer's hidden states</li>
<li>If guess is correct, speeds up next layer inference</li>
<li>If incorrect, can load actual next layer's experts later</li>
<li>Uses next layer's gating function applied to previous layer's hidden states</li>
<li>Relies on transformer layers being residual (each layer adds to previous hidden states)</li>
</ul>
<h3 id="llm-in-a-flash-efficient-llm-inference-with-limited-memory">[LLM-in-a-Flash] Efficient LLM Inference with Limited Memory<a class="zola-anchor" href="#llm-in-a-flash-efficient-llm-inference-with-limited-memory" aria-label="Anchor link for: llm-in-a-flash-efficient-llm-inference-with-limited-memory">üîó</a></h3>
<p>Arxiv: <a href="https://arxiv.org/abs/2312.11514">https://arxiv.org/abs/2312.11514</a> <em>12 Dec 2023 <strong>Apple</strong></em></p>
<p>Key Techniques:</p>
<ol>
<li>Windowing: Strategically reduces data transfer by reusing previously activated neurons</li>
<li>Row-column bundling: Tailored to sequential data access strengths of flash memory, increases size of data chunks read from flash memory</li>
</ol>
<h3 id="rope-roformer-enhanced-transformer-with-rotary-position-embedding">[RoPE] RoFormer: Enhanced Transformer with Rotary Position Embedding<a class="zola-anchor" href="#rope-roformer-enhanced-transformer-with-rotary-position-embedding" aria-label="Anchor link for: rope-roformer-enhanced-transformer-with-rotary-position-embedding">üîó</a></h3>
<p>Arxiv: <a href="https://arxiv.org/abs/2104.09864">https://arxiv.org/abs/2104.09864</a> <em>20 Apr 2021 <strong>Zhuiyi Technology Co.</strong></em></p>
<p>We investigated existing approaches to relative position encoding and found they are mostly built based on adding position encoding to context representations. We introduce Rotary Position Embedding (RoPE) to leverage positional information in PLMS learning. The key idea is to encode relative position by multiplying context representations with a rotation matrix with clear theoretical interpretation.</p>
<h3 id="speculative-fast-inference-from-transformers-via-speculative-decoding">[Speculative] Fast Inference from Transformers via Speculative Decoding<a class="zola-anchor" href="#speculative-fast-inference-from-transformers-via-speculative-decoding" aria-label="Anchor link for: speculative-fast-inference-from-transformers-via-speculative-decoding">üîó</a></h3>
<p>Arxiv: <a href="https://arxiv.org/abs/2211.17192">https://arxiv.org/abs/2211.17192</a> <em>30 Nov 2022 <strong>Google</strong></em></p>
<p>Key Observations:</p>
<ul>
<li>Some inference steps are &quot;harder&quot; and some are &quot;easier&quot;</li>
<li>Inference from large models is often bottlenecked on memory bandwidth and communication</li>
<li>Additional computation resources might be available</li>
</ul>
<p>Solution:</p>
<ul>
<li>Increase concurrency as complementary approach to adaptive computation</li>
<li>Accelerate inference without:
<ul>
<li>Changing model architectures</li>
<li>Modifying training procedures</li>
<li>Re-training models</li>
<li>Changing model output distribution</li>
</ul>
</li>
<li>Accomplished via speculative execution</li>
</ul>
<h3 id="gqa-training-generalized-multi-query-transformer-models-from-multi-head-checkpoints">[GQA] Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints<a class="zola-anchor" href="#gqa-training-generalized-multi-query-transformer-models-from-multi-head-checkpoints" aria-label="Anchor link for: gqa-training-generalized-multi-query-transformer-models-from-multi-head-checkpoints">üîó</a></h3>
<p>Arxiv: <a href="https://arxiv.org/abs/2305.13245">https://arxiv.org/abs/2305.13245</a> <em>22 May 2023 <strong>Google</strong></em></p>
<h3 id="multi-heads-sharing-fast-transformer-decoding-one-write-head-is-all-you-need">[Multi-Heads Sharing] Fast Transformer Decoding: One Write-Head is All You Need<a class="zola-anchor" href="#multi-heads-sharing-fast-transformer-decoding-one-write-head-is-all-you-need" aria-label="Anchor link for: multi-heads-sharing-fast-transformer-decoding-one-write-head-is-all-you-need">üîó</a></h3>
<p>Arxiv: <a href="https://arxiv.org/abs/1911.02150">https://arxiv.org/abs/1911.02150</a> <em>6 Nov 2019 <strong>Google</strong></em></p>
<p>Multi-head attention layers are a powerful alternative to RNNs for moving information across and between sequences. While training is generally fast and simple due to parallelizability, incremental inference is often slow due to memory-bandwidth costs of loading large &quot;keys&quot; and &quot;values&quot; tensors.</p>
<p>Solution: Multi-query attention</p>
<ul>
<li>Keys and values are shared across all attention &quot;heads&quot;</li>
<li>Greatly reduces size of tensors and memory bandwidth requirements</li>
<li>Much faster to decode with minor quality degradation</li>
<li>Identical to multi-head attention except heads share single set of keys and values</li>
</ul>
<h3 id="moe-outrageously-large-nn-the-sparsely-gated-moe-layer">[MoE] Outrageously Large NN: The Sparsely-Gated MoE Layer<a class="zola-anchor" href="#moe-outrageously-large-nn-the-sparsely-gated-moe-layer" aria-label="Anchor link for: moe-outrageously-large-nn-the-sparsely-gated-moe-layer">üîó</a></h3>
<p>Arxiv: <a href="https://arxiv.org/abs/1701.06538">https://arxiv.org/abs/1701.06538</a> <em>23 Jan 2017 <strong>Google</strong></em></p>
<p>Key Points:</p>
<ul>
<li>Neural network capacity is limited by number of parameters</li>
<li>Conditional computation (parts active per-example) proposed to increase capacity</li>
<li>Introduces Sparsely-Gated Mixture-of-Experts layer (MoE)</li>
<li>Consists of up to thousands of feed-forward sub-networks</li>
<li>Trainable gating network determines sparse combination of experts per example</li>
<li>Applied convolutionally between stacked LSTM layers</li>
<li>Achieves better results than state-of-the-art at lower computational cost</li>
</ul>
<h3 id="moe-moe-meets-instruction-tuning-a-winning-combination-for-llm">[MoE] MoE Meets Instruction Tuning: A Winning Combination for LLM<a class="zola-anchor" href="#moe-moe-meets-instruction-tuning-a-winning-combination-for-llm" aria-label="Anchor link for: moe-moe-meets-instruction-tuning-a-winning-combination-for-llm">üîó</a></h3>
<p>Arxiv: <a href="https://arxiv.org/abs/2305.14705">https://arxiv.org/abs/2305.14705</a> <em>24 May 2023 <strong>Google</strong></em></p>
<p>Sparse Mixture-of-Experts (MoE) is a neural architecture design that can be utilized to add learnable parameters to Large Language Models (LLMs) without increasing inference cost.</p>


                </div>
            </div>

            <div class="prev-link">
                
    
        
        
        <a class="previous" href="https://gel.github.io/generative-ai/2-llm-research/"><</a>
    

            </div>

            <div class="next-link">
                
    
        <a class="next" href="https://gel.github.io/generative-ai/2-llm-research/4-llm-prompting/">></a>
    

            </div>
        </div>

        
            
            <script type="text/javascript" src="https://gel.github.io/elasticlunr.min.js"></script>
            <script type="text/javascript" src="https://gel.github.io/search_index.en.js"></script>
            
            <script type="text/javascript" src="https://gel.github.io/book.js"></script>
            <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        

    </main>
    <footer class="footer-page">
    
      
    
    </footer>
  </body>
</html>
